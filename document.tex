\documentclass[5p,authoryear]{elsarticle}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
%\usepackage[small]{caption}
\usepackage[font=small,subrefformat=parens]{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{url}
\usepackage[table]{xcolor}
\usepackage{hyperref}
\usepackage{dblfloatfix}
\usepackage{adjustbox}

\journal{NeuroImage}

\bibliographystyle{elsarticle-harv}

\begin{document}

\begin{frontmatter}

\title{Identifying brain regions associated with virtual environment stimulus information}

\author[ECE,NS]{Andrew Floren}

\author[NS]{Bruce Naylor}

\author[CS]{Risto Miikkulainen}

\author[BCM]{David Ress\corref{cor}}
\ead{ress@bcm.edu}

\address[ECE]{Department of Electrical and Computer Engineering}
\address[NS]{Department of Neuroscience}
\address[CS]{Computer Science Department \\ The University of Texas at Austin, Austin, TX 78712 USA}
\address[BCM]{Department of Neuroscience \\ Baylor College of Medicine, Houston, TX 77030 USA}

\cortext[cor]{Corresponding author.}

\begin{abstract}
Virtual environments provide a framework for creating complex and dynamic stimuli in a controlled fashion while multivariate pattern analysis (MVPA) makes it possible to characterize the neural-processing substrate when the underlying processing model is unknown. 
Used together, virtual environments and MVPA allow for the creation of very powerful experimental designs. 
The benefits and potential pitfalls of these techniques are evaluated on the passive encoding of the number of human characters presented in a realistic and dynamic stimulus while functional magnetic resonance imaging (fMRI) data were analyzed using MVPA. 
Classifiers were trained using a spatially distributed subset of stimulus-responsive voxels at each time point and their performance was estimated using multi-fold cross-validation.
A sensitivity analysis was used to identify small and spatially distributed subsets of voxels that were sufficient for decoding the number of human characters from the fMRI signal.
The discriminative models learned by both the SVM and NN had a prediction accuracy that was three to four times above chance for all subjects, demonstrating that the combination of virtual environments and MVPA is a powerful approach for uncovering the neural circuitry in complex visual tasks.
\end{abstract}

\begin{keyword}
fMRI \sep MVPA \sep machine learning \sep virtual environments\sep vision
\end{keyword}

\end{frontmatter}

\section{Introduction}
Much research using functional magnetic resonance imaging (fMRI) and the blood-oxygen-level dependent (BOLD) signal utilize static images to analyze how the brain responds to stimuli.
Such work usually focuses on a single variable of interest, and creates stimuli that attempt to isolate that variable.
While this approach has proven to be successful, it does not mimic the dynamically changing environment in which the primate brain has evolved.
More natural and dynamic stimuli will evoke a more complex network of brain responses.
The context in which a stimuli is presented will affect how it is processed, and such complex responses can make interpretation difficult.
Therefore, it is important to test whether currently accepted neural correlates of experience remain valid in more natural settings.

Researchers have been using dynamic stimuli in the form of virtual environments \citep{Maguire1998,Calhoun2002,King2006,Mathiak2006,Spiers2007a,Hassabis2009} and prerecorded movies \citep{Hasson2004,Chadwick2010,Nishimoto2011} for some time.
Virtual environments have two strong advantages over prerecorded movies: the stimulus designer has complete control over the stimuli and the subject can interact with the environment in real time.
Precise control of the environment allows for the interrogation of very subtle parameters as well as automatic coding of relevant scene parameters.
Many variables of interest, such as semantic labels for visible objects are explicitly specified by the virtual environment's design, avoiding the need to manually label each frame (as is usually done with movies).
Furthermore, subjects interact with virtual environments in real time, which makes it possible to examine learning mechanisms and decision making in a realistic context.
This interactivity can also be employed to intentionally engage and control cognitive function.

\begin{figure*}
\centering
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/stimulus-town}
\caption{}
\label{fig:stimulus-town}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/stimulus-five-soldiers}
\caption{}
\label{fig:stimulus-five-soldiers}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/stimulus-three-insurgents}
\caption{}
\label{fig:stimulus-three-insurgents}
\end{subfigure}
\caption{
The stimulus in the experiment described in this paper employs a virtual environment and a blocked design where the view alternates between moving through the environment and viewing groups of animated characters.
\subref{fig:stimulus-town} An example frame from the stimulus where the camera is traveling through the virtual environment with no characters presented.
\subref{fig:stimulus-five-soldiers} An example frame from the stimulus where five friendly characters are being presented.
\subref{fig:stimulus-three-insurgents} An example frame from the stimulus where three hostile characters are being presented.
Such stimuli allow studying how the brain respondes in a more natural and complex environment.
}
\label{fig:stimulus}
\end{figure*}

More complex stimuli require more complex analysis methods.
A wide variety of methods have been used to analyze dynamic stimuli; general linear models (GLM) including statistical parametric mapping (SPM), independent component analysis (ICA), reverse correlation, and multivariate pattern analysis (MVPA) are the most popular \citep{Spiers2007}.
These analysis methods can generally be split into stimulus-driven methods---such as SPM---that measure the statistical significance of a hypothesis, and data-driven methods---such as MVPA---that look for patterns in the fMRI signal.
Stimulus-driven analyses provide more easily interpretable results but become less tractable as the complexity of the stimulus increases.
Data-driven analyses, on the other hand, can cope with more complex stimuli but are more difficult to relate to brain physiology.
A hybrid approach is also possible: data-driven analyses can first uncover complex patterns evoked by realistic stimuli; the observed relationships can then help formulate hypotheses and design stimuli to be tested by stimulus-driven analyses. 
However, it may also be possible to use these realistic stimulus-response relationships to understand subject training and pathophysiology better.

Although many dynamic stimuli experiments employ GLM methods \citep{Maguire1998,Calhoun2002,King2006,Mathiak2006,Spiers2007a}, there is a general trend in newer experiments to use MVPA \citep{Hassabis2009,Chadwick2010}. 
The application of MVPA analysis to virtual environments stimuli opens up an exciting range of experiments to examine complex brain responses in more realistic stimulus scenarios.
MVPA has been used as a blanket term for the application of any classification algorithm to fMRI data, although the support vector machine (SVM) is by far the most popular choice.
When applying MVPA, the stimulus is typically organized into multiple categories or classifications such as houses or faces, ambiguous or unambiguous sentences, or gratings of specific orientations \citep{Haxby2001,Mitchell2003,Haynes2006}.
The algorithm is then trained to classify the patterns of BOLD activations into these categories at a particular time-point using labeled examples.
The performance of the classifier is defined to be the probability that it will classify a previously unseen example successfully;
this measure of performance must be estimated using statistical methods.
MVPA can be applied to the whole brain as well as to small localized regions (e.g., using the ``searchlight'' technique; \cite{Kriegeskorte2006}), which can then be used to localize functional processing.
MVPA methods consider the pattern of activation of many voxels at a particular interval in time, in contrast to GLM, which treats each voxel independently as a time-series of responses over an extended period of time.
By considering multiple voxels simultaneously, MVPA has been used to identify effects that were previously thought to be too fine scale to resolve with fMRI \citep{Kamitani2005,Hassabis2009}.

In this paper, the feasibility of extracting complex information from fMRI data while subjects view natural and dynamic stimuli is tested.
The experiment were structured to identify those parts of the brain that contain information about the number of human characters in a scene.
Unlike previous experiments that examined the neural correlates of object counting or number evaluation \citep{Dehaene1999,Rickard2000,Barth2006}, a passive-viewing virtual environment stimulus paradigm was used, and the fMRI data was analyzed using MVPA.
Classifiers were trained using a spatially distributed subset of stimulus-responsive voxels at each time point and their performance was estimated using multi-fold cross-validation. 
A variety of temporal splitting schemes were carefully examined to assess their effect on classifier performance.
Even for the most conservative splitting scheme, classification accuracy was three to four times better than chance for all subjects and both machine-learning techniques tested.
A novel sensitivity mapping technique was then used to identify voxels that are important for classification and thus likely to be involved in processing the information that the classifiers were trained to identify.
Results indicate that information regarding  the number of visible characters in a complex stimulus is largely encoded early in visual processing, utilizing both ventral and dorsal visual streams.
The results demonstrate that complex information can effectively be extracted from a realistic virtual environment stimulus paradigm, opening up new prospects for the use of virtual environments with MVPA in neuroscience.

\section{Methods}

\subsection{Subjects}
Five adult males, with normal or corrected-to-normal vision, participated in the experiments. 
All subjects participated in two fMRI sessions and a third session to acquire a high-resolution structural anatomy. 
Informed consent was obtained from all subjects.

\subsection{Stimulus}
To facilitate designing our virtual environment, the Unreal Developer's Kit developed by Epic Games, Inc. was used (available at \url{http://www.unrealengine.com/udk}).
This development kit is available free of charge for non-commercial applications and uses the same rendering and game engine found in many current and popular video games.

One of the long-term goals of this research is to understand post-traumatic stress disorder (PTSD).
Therefore, we created a virtual town intended to mirror the kinds of real-world settings encountered by many currently deployed military forces (Fig. \ref{fig:stimulus}).
Virtual characters representing friendly forces and hostile combatants were situated at a variety of locations throughout the virtual town.
The stimulus was rendered in real-time from the point of view of a camera moving at eye level through the town. 

During the stimulus presentation, the camera followed a preset path through the town.
The stimulus employed a blocked approach. The camera would be steadily moving for 15 seconds, followed by 15 seconds in which it stopped to view virtual characters at predefined locations.
There were four different predefined character-viewing locations, and the number of characters present at each of these locations varied from one to six (Figs. \ref{fig:stimulus-five-soldiers} and \ref{fig:stimulus-three-insurgents}).
While the camera was stopped to view the characters, the view slowly panned and rotated and the characters engaged in animated movement sequences, so that the scene was never static.
A passive viewing paradigm was employed for this stimulus: there was no fixation dot and no task during the presentation.

Scanning sessions included five to six runs that were six minutes in duration.
Each run contained 12 alternations between moving through the virtual environment and character presentations. 
Since there were four predefined character viewing locations, the camera made three laps around the town during a single run.
While the number of characters presented in each block varied from one to six, the stimulus was controlled such that a presentation with a particular number of characters appeared twice in each run.
The ordering of the presentations was also controlled such that presentations with the same number of characters were located at different viewing points.
The order of the presentations was generated randomly given the previous constraints, but the same ordering was used for each subject.
A single type of character (friend or foe) was presented in each run, but the type alternated from run to run.

\subsection{MRI protocols}
Imaging was performed on a GE Signa Excite HD scanner using the product eight-channel head coil.
Whole-brain image volumes were collected using a custom GRAPPA EPI sequence \citep{Griswold2002}. 
Sequence parameters were g-factor = 2,  TE = 25 ms, TR = 2.5 s, and  2.5-mm cubic voxels across a 200 mm field-of-view. 
The slice prescription included 40 slices oriented along the AC-PC axis. 
A high-order shim was  performed to improve field homogeneity.

A set of T1-weighted structural images was obtained on the same prescription at the end of functional acquisition session using a three-dimensional (3D) fast RF-spoiled GRASS (fSPGR) sequence. 
These anatomical images were then used to align the functional data to a structural 3D reference volume, which was acquired for each subject in a separate session. 
The structural reference volume was T1-weighted with good gray-white contrast and was acquired using a 3D inversion-prepared fSPGR sequence (minimum TE and TR, TI = 450 ms, 15$^\circ$ flip angle, isometric voxel size of 0.7 mm, 2 excitations, $\sim$28-minute duration).

%\begin{figure}
%\centering
% \begin{subfigure}{0.3\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/hrf}
% \caption{}
% \label{fig:wiener-hrf}
% \end{subfigure}
%\begin{subfigure}{0.4\textwidth}
%\centering
%\includegraphics[width=\textwidth]{figures/square-wiener-deconvolution}
%\caption{}
%\label{fig:wiener-square}
%\end{subfigure}
%\begin{subfigure}{0.4\textwidth}
%\centering
%\includegraphics[width=\textwidth]{figures/voxel-wiener-deconvolution}
%\caption{}
%\label{fig:wiener-voxel}
%\end{subfigure}
%\caption{
%\subref{fig:wiener-hrf} The difference-of-gamma HRF employed as $h(t)$. 
%Wiener filter deconvolution is an optimal method for obtaining neural activity from the BOLD response.
%Most importantly, such deconvolution aligns neural activity properly with the stimulus presentation in time.
%\subref{fig:wiener-square} A simple square wave in red, the same square wave after convolution with a canonical HRF in green, and after Wiener-filter deconvolution in blue. 
%\subref{fig:wiener-voxel} An example voxel time series in red and the same time series after Wiener-filter deconvolution in blue.
%Wiener filter deconvolution removes the delay introduced by the HRF so that patterns of activation are properly associated with the stimulus that produced them.}
%\label{fig:wiener-deconvolution}
%\end{figure}

\subsection{Preprocessing}
Preprocessing of the fMRI data was performed using the mrVista software package (available at \url{http://vistalab.stanford.edu/}). 
The first 15 seconds of data  were discarded to reduce transient effects.
Within-scan motion was then estimated using a robust intensity-based scheme \citep{Nestares2000}. 
Between-run motion was corrected using the same scheme, this time applied to the temporal average intensity of the entire scan. 
The first run of the session was used as the reference. 
Additionally, a Wiener filter deconvolution \citep{Poor1980} was applied using a generic difference-of-gamma hemodynamic response function (HRF; \cite{Glover1999}) as the kernel to the recorded BOLD signal.
Mostly, the deconvolution served to shift the peak response in time so that it was aligned with its associated stimulus, but it also provided some amount of noise reduction.
Because the goal is to learn associations between patterns of activation in the brain and stimulus presentation, it is important that the activation is temporally aligned with the stimulus.

In Wiener filter deconvolution, the deconvolution kernel $g(t)$ is most easily expressed in the Fourier domain as
\begin{equation}
g(t) \xrightarrow{\mathcal{F}} \frac{H^{*}(f)}{\left|H(f)^{2}\right| + \frac{\left| N(f) \right|}{\left| X(f) \right|}},
\end{equation}
where $h(t)$ is the blurring kernel, $x(t)$ is the signal of interest, and $n(t)$ is additive noise; capital letters denote the Fourier transform of each quantity, e.g., $h(t) \xrightarrow{\mathcal{F}} H(f)$.

In fMRI, $h(t)$ is the hemodynamic response function and $x(t)$ is the neural response.
Calculating $g(t)$ requires estimates of the power spectral density of the signal of interest as well as the noise.
However, the noise $n(t)$ corresponds not only to scanner noise but other nuisance factors such as pulse and respiration.
These factors make modeling the noise, and its power spectral density, very difficult.
Therefore, $\frac{\left| N(f) \right|}{\left| X(f) \right|}$ was set to $1.0$, a compromise value that provided a satisfactory combination of temporal alignment and noise reduction for all subjects.

The high-resolution reference anatomies were segmented using the Freesurfer image analysis suite (\url{http://surfer.nmr.mgh.harvard.edu/}) to create approximate parcellations of the gray matter in each subject, as well as a surface model useful for visualization of the results.

\subsection{Dimensionality reduction}
When performing MVPA, each voxel can be thought of as corresponding to a separate dimension of a very high-dimensional space.
For this experiment's acquisition parameters, the number of voxels/dimensions was $80 \times 80 \times 40 = 256,000$.
Unfortunately, the number of examples necessary to train a classifier increases rapidly with the number of dimensions, so it was necessary to reduce this dimensionality.
Such reduction not only speeded up both training and classification but also improved the performance of the resulting classifiers.

Principal component analysis (PCA; \cite{Hotelling1933}) is a common tool for dimensionality reduction.
However, PCA only selects the orthogonal dimensions with the highest variance, which is likely to be physiological nuisance and is therefore not well suited for fMRI analysis.
Univariate statistical methods, as typically used in  fMRI analysis, can also be good candidates for selecting voxels \citep{Norman2006,Pereira2009}.
While effective, these methods were not designed for feature selection and many of them make a number of assumptions about the data such as Gaussian statistical distribution.

A novel method of dimensionality reduction was developed in this research.
This method, called harmonic analysis, selects voxels that are driven most strongly by the 30-second duty cycle of the block design.
This method is similar to other univariate statistics, but the only assumption is that the BOLD response is linear with respect to the stimulus.
The primary advantage is that it can detect voxels that covary with the stimulus regardless of their detailed temporal relationship. 
Thus, it includes voxels that respond positively to either the character presentation or motional phases of the stimulus alternation, or to more complex patterns such as a brief strong response to both phases. 
All repetitive responses that follow the period of the stimulus alternation are included by this method, minimizing any bias in the dimension reduction.

Harmonic analysis takes advantage of the fact that the response of any linear system to a blocked alternation at frequency $f$ will contain power only at $f$ and its harmonics. 
Under a linear response assumption, we can therefore form an unbiased estimate of the response power by summing the power at these frequencies. 
Let $y(t)$ be the recorded discrete time series at some voxel.
Then let $Y(f)$ be the discrete Fourier transform of $y(t)$.
The fractional harmonic power of that time series is defined as
\begin{equation}
P_h = \frac{\sum_{i = 1}^{M}{\left|Y(i \cdot N)\right|^{2}}}{\sum_{f}{\left|Y(f)\right|^{2}}},
\end{equation}
where $P_h$ is the fractional harmonic power, $M$ is the number of harmonics, and $N$ is the frequency of interest, in our case the period of the block alternations. 
Because the BOLD response has a predominantly low-pass temporal frequency response, $M = 4$ is sufficient. 
Using $P_h$, a particular number $N$ (typically 2000) voxels with the greatest power are then selected. 

This harmonic-power selection was based on the alternation between characters present and characters absent, without regard to the number of characters presented. 
Therefore, classifier accuracy estimates will only be presented for character count, and not for the presence or absence of characters, in order to avoid cross-contamination between dimension-reduction and  classification criteria that would result in inflated classifier performance estimates \citep{Pereira2009}.
However, as a check the classifiers were also trained to distinguish between time points with and without characters, and their performance was consistently above 95\%, confirming that the machine-learning algorithms were working correctly.

\subsection{Classification}
Using the time series from the voxels selected by the harmonic analysis, a one-versus-one multi-class linear support vector machine (SVM) \citep{Cortes1995,Weston1999}, a feedforward neural network (NN) \citep{Hornik1989,Hagan1994}, a Gaussian naive Bayes classifier (GNB) \citep{Duda1973}, and a k-nearest neighbor classifier (KNN) \citep{Cover1967} were trained to identify the number of characters presented in each scene, regardless of character type (friend or foe).
To maximize temporal resolution, each 2.5-second frame (time point) was treated as a separate example, rather than aggregating across the 15-second blocks.
Similar methods were also used to classify scenes as friends or foes, but these classifiers did not perform significantly above chance and will not be mentioned further.

To measure the significance of the results and to compare the classifiers a performance measure is needed.
The performance of machine learning algorithms is generally defined to be the expected accuracy of the classifier on previously unseen examples \citep{Bishop2006}.
In practice, this measure can only be estimated.
A typical approach is to split the available examples into training and test sets.
The classifier is first trained on the training set, and its performance on the test set is then taken as the estimate.
In practice, the process of splitting all of the available examples into training and test sets is performed multiple ($n$; typically 10) times to reduce the variance of the performance estimate.
This procedure is known as $n$-fold cross-validation \citep{Kohavi1995} where each split of the data into training and test sets is called a ``fold''.

Furthermore, the null distribution for the cross-validated performance estimates was generated by randomly permuting the labels on the examples 2000 times and repeating the training and cross-validation procedure.
That is, the distribution of performance estimates was generated under the assumption that the labels and data were independent.
Using this distribution, $p$ values were calculated for the performance estimates.
The resources of the Texas Advanced Computing Center (TACC) at The University of Texas at Austin were leveraged to perform this computation.

Previous studies have discussed issues with optimistically biased performance estimates due to temporal correlations that violate independence assumptions between training and test set samples \citep{Pereira2009}. 
The slow speed of the fMRI hemodynamic response clearly introduces temporal correlations on time scales less than 15 seconds.
Artificial correlations therefore occur in the data when the random sampling that divides the data into the different sets yields training and test samples from the same (15 sec) block.

This issue raises a more general question: What is the relationship between performance estimates and temporal correlation?
To answer this question, classifier performance was estimated using four different methods for splitting the training and test examples, in order to vary the temporal distance between frames in the two sets. 
In the first method, referred to as  ``frame split", frames were independently drawn into the training and test sets. 
Although the draws were independent, there is no restriction to prevent adjacent frames from being split between the training and test sets.
This method should exhibit artificially high performance because of the temporal correlation.
In the second method, referred to as ``block split", sets of frames corresponding to the 15 second blocks were independently drawn into the training and test sets.
The third method exploits the fact that each six minute run cycles through all combinations twice, but in a different order the second time.
Accordingly, sets of frames were independently drawn from these half-runs, referred to as ``half-run split". 
In the fourth method, termed ``run split", entire six-minute runs of frames were independently drawn. 
Table \ref{tab:training-split} summarizes these training and test set split methods and the temporal separations between the training and test data for single runs. 
The mean temporal separations do not vary much between the various splits because the randomization extends across the entire run. 
However, the minimum temporal distance between a frame in the test set and any frame in the training set is a more relevant measure for comparison: It doesn't matter that on average the examples are highly separated temporally if on average there is at least one example in the training set which was temporally close to each example in the test set.

%\begin{figure*}
%\centering
%\begin{subfigure}{0.3\textwidth}
%\centering
%\includegraphics[width=\textwidth]{figures/mean-delay-graph}
%\caption{}
%\label{fig:mean-delay-graph}
%\end{subfigure}
%\begin{subfigure}{0.3\textwidth}
%\centering
%\includegraphics[width=\textwidth]{figures/min-delay-graph}
%\caption{}
%\label{fig:min-delay-graph}
%\end{subfigure}
%\caption{The \subref{fig:mean-delay-graph} mean delay and \subref{fig:min-delay-graph} minimum mean delay between examples in the train and test set for four of the split methods. 
%The session split method is excluded because both the mean and minimum delay are much larger than the other splits and the delays vary significantly between subjects.}
%\end{figure*}

\begin{table}
\centering
\input{tables/temporal-delay.tex}
\caption{A summary of the temporal delay between the different training-and-test-split methods.
There is very little difference in the average temporal delay between examples.
However, the average minimum delay between any example in the training set and an example in the test set varies considerably.
This average minimum delay is the important quantity to consider when trying to avoid optimistic performance estimates.}
\label{tab:training-split}
\end{table}

For the frame and block splits, classifier performance was estimated using ten-fold cross-validation.
For the half-run and run splits, only eight and four unique splits are possible, respectively, due to the much smaller number of runs per subject. 
Therefore, only eight-fold and four-fold cross-validation were employed for estimating classifier performance on these splits.

% We can also discuss classifier performance with respect to a single class.
% Single class performance is traditionally measured along two axes: \emph{precision} and \emph{recall}.
% \emph{Precision} with respect to class $c$ is the probability that a previously unseen data point was classified correctly given that it was classified as $c$.
% \emph{Recall} with respect to class $c$ is the probability that a previously unseen data point was classified correctly given that it is actually a member of $c$.
% Again, these measures can only be estimated from the test set.
% $\mbox{\emph{precision}} = tp / (tp + fp)$, where $tp$ is the number of true positives and $fp$ is the number of false positives.
% $\mbox{\emph{recall}} = tp / (tp + fn)$, where $fn$ is the number of false negatives.

% Another tool for examining classifier performance is the confusion matrix.
% If $\mathbf{C}$ is a confusion matrix, then the value of $C_{ij}$ is equal to the number examples of class $i$ that were classified as class $j$.
% Therefore, values along the diagonal of a confusion matrix correspond to correct classifications while other values correspond to incorrect classifications.
% The confusion matrix also simplifies estimating precision and recall for each class.
% The value of $C_{ii}$ divided by the sum of all values along row $i$ is the estimate for the recall of the $i^{th}$ class.
% Similarly, the value of $C_{jj}$ divided by the sum of all values along column $j$ is the estimate for the precision of the $j^{th}$ class.
% Finally, overall classifier performance can be estimated by dividing the sum along the diagonal, or the trace, by the sum of the entire matrix.

\subsection{Sensitivity Analysis}
MVPA can tell us whether the time-series data from a subset of human brain voxels can be used to discriminate from the stimuli. 
However, these results do not show which voxels in the large group were actually important for that discrimination.
This information is important for localizing functions in the brain.
One existing technique is to train machine learning classifiers on small localized areas in the brain and use their performance as a measure of the strength of the function in question in that area; this is known as the ``searchlight'' technique \citep{Kriegeskorte2006}.
While this technique is effective for simple highly-localized functions, the results are less clear when the function is sparsely distributed over the brain.
A single region may not contain enough information for accurate classification or a region may only contain relavant information when considered in conjunction with a spatially disparate region.

\begin{figure*}
\centering
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/lh-lateral-labels}
\caption{lateral labels}
\label{fig:leteral-labels}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/lh-medial-labels}
\caption{medial labels}
\label{fig:medial-labels}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/sensitivity-legend-crop}
\end{subfigure}
\caption{After generating sensitivity maps, region labels were used for aggregating the results, as shown in table \ref{tab:full-sensitivity}. In this manner, it was possible to obtain a clearer representation of the pattern of sensitivity.}
\label{fig:labels}
\end{figure*}

To overcome this limitation, a sensitivity analysis technique originally used to minimize the input data dimension of feedforward neural networks \citep{Zurada1994} was adapted to display the spatially distributed set of voxels that are most important for identifying the classes.
Specifically, the sensitivity, or magnitude of change, of the neural network output was calculated with respect to a change in each voxel.
Let $\mathbf{o}$ be the vector of outputs and $\mathbf{x}$ be the vector of inputs.
Then the sensitivity of output $k$ to input $i$ is defined by
\begin{equation}
S_{ki} = \frac{\delta o_{k}}{\delta x_{i}},
\end{equation}
which is the partial derivative of the output with respect to the input.
Let $\mathbf{w}$ be the weight matrix from the hidden layer to the output layer and $\mathbf{v}$ be the weight matrix from the input layer to the hidden layer.
Then the partial derivative can be expressed as
\begin{equation}
\frac{\delta o_{k}}{\delta x_{i}} = o'_{k} \sum^{J}_{j=1}{w_{kj}y'_{j}v_{ji}},
\end{equation}
where $J$ is the total number of hidden units in that layer of the neural network,  $o'_{k}$ is the value of the derivative of the activation function at output $k$, and $y'_{j}$ is the value of the derivative of the activation function at hidden neuron $j$.
Finally, the entire sensitivity matrix can be expressed in matrix notation as,
\begin{equation}
\mathbf{S} = \mathbf{O}' \times \mathbf{W} \times \mathbf{Y}' \times \mathbf{V}
\end{equation}
where
\begin{equation}
\mathbf{O}' = diag(o'_{1},~o'_{2},~\cdots,~o'_{K})
\end{equation}
\begin{equation}
\mathbf{Y}' = diag(y'_{1},~y'_{2},~\cdots,~y'_{K}).
\end{equation}
However, because the transfer functions are nonlinear they can only be evaluated for specific input values.
Therefore, the average sensitivity matrix across all input vectors,
\begin{equation}
\mathbf{S}_{avg} = \sqrt{ \frac{ \sum_{n = 1}^{N}{ \left( \mathbf{S}_{n}\right)^{2} } }{N} },
\label{eqn:sensitivity}
\end{equation}
where $N$ is the number of input vectors, is calculated.
The magnitude is squared so that the effects of both positive and negative sensitivities are included in the average.
Eq. \ref{eqn:sensitivity} gives a sensitivity value for each voxel with respect to every output, whereas it is useful to have a measure of the sensitivity of a voxel with respect to any output.
Such a metric can be defined by taking the maximum sensitivity of each voxel across all outputs, i.e.
\begin{equation}
\Phi_{i} = \max_{k=1 \dots K}{S_{ki,~avg}}.
\end{equation}
This sensitivity can now be projected back into the volume anatomy space to create a  map that reflects the relative incremental importance of each voxel's response to the classification decision.
In theory, a similar technique could be introduced for any differentiable classifier such as the SVM.
However, only the sensitivity analysis applied to neural networks will be presented and discussed as it has been well studied in the existing literature \citep{Zurada1994}.

To  explore the relationship between sensitivity and classifier performance, the performance of the classifier was plotted when trained on only a subset of the input voxels as determined by a minimum-sensitivity cut off.
This threshold was determined by iteratively retraining the neural network with successively higher thresholds.

The Freesurfer image analysis suite was employed to construct cortical surfaces for each of the subjects using high-resolution anatomy volumes.
Then, 10 anatomical labels were constructed on each surface based on FreeSurfer's automatically generated labels (Figure \ref{fig:labels}).
The labels largely overlap with FreeSurfer's labels.
However, the banks of the superior temporal were incorporated into the superior and middle temporal labels.
Additionally, the supramarginal label was merged with the inferior parietal label.
For most subjects, several of the automatic labels were also edited by hand to conform better to the actual sulcal boundaries.
Each subject's volume sensitivity map was then projected onto their cortical surface and blurred along the surface using a 5 mm full-width half-maximum (FWHM) Gaussian kernel.

\subsection{GLM}
GLM analysis was also performed on the data for comparison purposes as well as confirmation of the results of the sensitivity analysis. 
fMRI data processing was carried out using FEAT (FMRI Expert Analysis Tool) Version 5.98, part of FSL (FMRIB's Software Library, \url{www.fmrib.ox.ac.uk/fsl}). 
Z (Gaussianized T/F) statistic images were thresholded using clusters determined by $Z > 2.3$ and a (corrected) cluster significant threshold of $P = 0.05$ \citep{Worsley2001}.
A linear activation hypothesis was constructed based on the results from the sensitivity analysis.
For each voxel above the significance threshold, this model was used to construct a simple linear estimator for the number of characters presented.
The total accuracy of this model was calculated for comparison against the machine learning techniques.
It should be noted that cross-validation was not employed and therefore the accuracy estimates will be highly optimistic.
The thresholded Z statistic images were projected onto the same Freesurfer generated surfaces as the sensitivity analysis.

\section{Results}
Cross-validated performance estimates of all four classifiers are plotted against the four training-and-test-split methods corresponding to four different average minimum temporal delays (Figure \ref{fig:performance-verse-temporal-distance}).
The SVM had the best performance overall followed by the feedforward neural network.
There is some variation of classifier performance between subjects, but in all cases the performance is significantly ($p < 0.001$) above chance ($16.\overline{6}$\%). 
The performance of all four independent classifiers being above chance increases confidence in the results, however the GNB and KNN classifiers will not be discussed further as their performance was significantly below the SVM and NN (Figure \ref{fig:performance-verse-temporal-distance}).
The average performance of the best linear estimator for each session after thresholding was 29\%.
This puts the linear estimator on par with the GNB and KNN classifiers.
However, do not forget that no cross-validation was performed and therefore this estimate is likely inflated.

As the temporal delay between data in the training and test sets increases, the estimated performance of the classifiers decreases somewhat.
However, the only statistically significant shift occurs between the frame split and block split, corresponding to 2.6 second and 21 second average minimum temporal distance between examples in the training and test sets, respectively.
Thus, as expected, optimistic performance estimates are driven by temporal correlations on the 15--20-s time-scale of the HRF.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{figures/performance-verse-temporal-distance}
\caption{The estimated performance of the classifiers averaged across all sessions and plotted across the four training-and-test-split methods, corresponding to the four different average minimum temporal delays between examples in the training and test sets. 
The performance estimates were bootstrapped across sessions in order to obtain 68\% confidence intervals.
There is a statistically significant drop in the estimated performance when the average minimum temporal delay increases from 2.6 to 21 seconds, confirming that temporal correlations result in optimistic performance estimates with short delays.}
\label{fig:performance-verse-temporal-distance}
\end{figure}

The average confusion matrix gives an intuitive look at the performance of the classifiers (Figure \ref{fig:average-confusion}).
The classifiers are much better at detecting the presence of a single character than any other count.
In fact, there are almost no cases of confusion between one and two characters.
Apparently, these two situations evoke very different responses in the brain.
The rest of the character counts are distinguished with relatively equal accuracy,
except for a slightly larger tendency to mis-classify the six character presentation.
Note that the majority of the incorrect responses lay just off the main diagonal.
These responses correspond to the classifier being wrong by a single character in its classification.
For example, in 21\% of the test examples, the machine learning algorithm classified a frame as containing four characters when it only contained three characters.
The estimated performance does not take the cardinality of the classes into account and considers mislabeling one character as two equivalent to mislabeling one character as six characters.
A soft measure that would do that might be useful but the results are harder to interpret.
A confusion matrix gives us the advantages of the soft measure while providing an easily interpretable look into the performance of the classifiers.

\begin{figure}
\centering
\begin{subfigure}{0.3\textwidth}
\centering
\begin{adjustbox}{center}
\input{tables/svm-confusion.tex}
\end{adjustbox}
\caption{}
\label{fig:average-confusion-svm}
\end{subfigure}
\\
\begin{subfigure}{0.3\textwidth}
\centering
\begin{adjustbox}{center}
\input{tables/nn-confusion.tex}
\end{adjustbox}
\caption{}
\label{fig:average-confusion-nn}
\end{subfigure}
\caption{The average confusion matrices for the \subref{fig:average-confusion-svm} SVM and \subref{fig:average-confusion-nn} neural network classifiers across all subjects using the block split.
The value in cell $(i,j)$ of the matrix is the percent of examples from class $i$ that were labeled as class $j$; percents along the diagonal indicate correctly classified examples while the rest indicate incorrectly classified examples.
The color of the cell indicates deviation from chance probability ($16.\overline{6}$\%).
The more green a cell is the further above chance that percent is, while the more red a cell is the further below chance that percent is.
These confusion matrices indicate that the SVM and NN were able to distinguish all six characters counts, but as the number of characters rose the performance tended to decrease.}
\label{fig:average-confusion}
\end{figure}

Thresholds for the sensitivity maps were determined using the iterative retraining approach outlined in the methods section.
Figure \ref{fig:sensitivity-cutoff} shows the performance estimate of the feedforward neural network averaged across subjects and the percent of input voxels used for training plotted against the sensitivity threshold value.
The estimated performance of the classifier was not significantly affected until over 90\% of the voxels were removed.
However, there is a downward trend in the performance after a peak when $\sim$35\% of the voxels are removed.
Interestingly, there is also an upward trend up to 35\%, presumably because noise can be effectively eliminated in this way.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{figures/performance-verse-sensitivity-cutoff}
\caption{A plot of the feedforward neural network estimated performance and the percent of voxels used for training when the inputs are pruned at a particular sensitivity value.
The fraction of voxels is calculated with respect to the 2000 voxels selected by harmonic analysis.
The performance estimates and voxel counts were bootstrapped across sessions in order to obtain 68\% confidence intervals.
The estimated performance is not significantly reduced until over 90\% of the voxels have been pruned.
Interestingly, low levels of pruning may actually improve performance (presumably by reducing noise), although this effect was not statistically significant in the current experiment.} 
\label{fig:sensitivity-cutoff}
\end{figure}

Sensitivity maps show a preponderance of classification sensitivity in lateral occipital areas, ventral retinotopic early visual areas, and dorsal parietal lobe (Figure \ref{fig:individual-sensitivity}). 
The Z statistic maps produced by GLM also indicate significant linear activation in ventral retinotopic early visual areas as well as some linear activation in lateral occipital regions.
Individual subjects also displayed small regions of high sensitivity in idiosyncratic portions of temporal and frontal cortex. 
Averaged across subjects, classifier sensitivity was most affected (in order of importance) by lateral occipital regions, lingual areas, and dorsal parietal lobe (Table \ref{tab:full-sensitivity}). 

\begin{figure*}
\centering
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s1-lh-lateral-sensitivity}
% \caption{}
% \label{fig:s1-lh-lateral-sensitivity}
% \end{subfigure}
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s1-lh-medial-sensitivity}
% \caption{}
% \label{fig:s1-lh-medial-sensitivity}
% \end{subfigure}
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s1-lh-lateral-zscore}
% \caption{}
% \label{fig:s1-lh-lateral-zscore}
% \end{subfigure}
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s1-lh-medial-zscore}
% \caption{}
% \label{fig:s1-lh-medial-zscore}
% \end{subfigure}
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s2-rh-lateral-sensitivity}
% \caption{}
% \label{fig:s2-rh-lateral-sensitivity}
% \end{subfigure}
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s2-rh-medial-sensitivity}
% \caption{}
% \label{fig:s2-rh-medial-sensitivity}
% \end{subfigure}
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s2-rh-lateral-zscore}
% \caption{}
% \label{fig:s2-rh-lateral-zscore}
% \end{subfigure}
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s2-rh-medial-zscore}
% \caption{}
% \label{fig:s2-rh-medial-zscore}
% \end{subfigure}
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s3-lh-lateral-sensitivity}
% \caption{}
% \label{fig:s3-lh-lateral-sensitivity}
% \end{subfigure}
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s3-lh-medial-sensitivity}
% \caption{}
% \label{fig:s3-lh-medial-sensitivity}
% \end{subfigure}
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s3-lh-lateral-zscore}
% \caption{}
% \label{fig:s3-lh-lateral-zscore}
% \end{subfigure}
% \begin{subfigure}{0.2\textwidth}
% \centering
% \includegraphics[width=\textwidth]{figures/s3-lh-medial-zscore}
% \caption{}
% \label{fig:s3-lh-medial-zscore}
% \end{subfigure}
\input{surface-maps.pdf_tex}
\caption{Here we present individual sensitivity maps and Z statistic maps of linear activation for three different subjects projected onto their semi-inflated cortical surfaces.
 The first row (figures a-d) present lateral and medial views of the sensitivity map (a-b) and Z statistic map (c-d) for the first subject.
 The second row (figures e-h) present lateral and medial views of the sensitivity map (e-f) and Z statistic map (g-h) for the second subject.
 The third row (figures i-l) present lateral and medial views of the sensitivity map (i-j) and Z statistic map (k-l) for the third subject.
The sensitivity maps are relatively homogeneous across subjects and hemispheres.}
\label{fig:individual-sensitivity}
\end{figure*}

\begin{table*}
\centering
\input{tables/full-sensitivity-table.tex}
\caption{Sensitivity map values integrated across the cortical surface labels from figure \ref{fig:labels}. Sensitivities are shown for each subject (\emph{A}--\emph{E}), and their mean values, as shown ordered from greatest to least sensitive brain region.}
\label{tab:full-sensitivity}
\end{table*}

\section{Discussion}
Although all classifiers performed at well above chance levels for all subjects, the average performance on individual subjects varied significantly.
These variations could be the result of differences in age, general cognitive state, or simply how much attention the subject was paying to the stimulus during a session.
The lack of a task during the stimulus presentation makes these variations difficult to interpret since attention and cognitive state were not well controlled.
In future experiments, the subjects will be asked to perform a challenging task, and the average performance is expected to increase and the variation between subjects to decrease.

This estimated prediction accuracy was calculated after correcting for the bias introduced by temporal correlations in the BOLD signal while minimizing variance.
By characterizing the relationship between the average minimum temporal distance between the training and test sets and estimated classifier performance, the estimated performance was found to reach a minimum around 20 seconds.
These findings are consistent with knowledge of the hemodynamic response.
That is, so long as the frames are separated by approximately the time-scale of the HRF then they can reasonably be considered independent, and this separation produces an appropriately conservative measure of classification performance. 
However, it is interesting that slightly better performance may be obtained for somewhat longer delays, although this trend was not statistically significant.
Many experiments separate their folds for crossfold-validation along runs.
While this procedure ensures a sufficient temporal separation between training and test set examples, it also reduces the total number of folds available and thus increases the variance of the estimate.
Instead, we recommend splitting the data along the shortest natural boundary of the experimental procedure that is not less than 20 seconds.

The classifier performances validate the popularity of the SVM with MVPA.
In general, better classifier performance leads to higher confidence in the results of MVPA.
Therefore, it is important to identify new classification algorithms that could offer superior performance.
Deep learning \citep{Hinton2006} is a recent neural network formulation that has shown potential for solving a variety of complex tasks \citep{Ciresan2012}.
While the standard feedforward neural network did not have the best performance, it was not far behind, which indicates that deep learning could outperform even the SVM.

Ideally, the classifiers should be trained using data from the whole brain so as not to miss any relevant interactions.
Since such training is prohibitively expensive, the harmonic analysis technique was developed in this work to perform dimensionality reduction without making any assumptions about the signal other than linearity.
While less appropriate for statistical analysis, this technique works well as a preprocessing step before MVPA.

The classifiers' performance indicates that there is sufficient information in the pattern of BOLD signals to decode character count in a freely viewed stimulus. 
The results also suggest that the neural substrates that encode such information are both distributed and redundant.
In order to explore where in the brain this information is encoded, a sensitivity mapping technique was developed.
This technique made it possible to identify a small distributed subset of voxels that are sufficient for decoding the character count.
Using dimensionality reduction via harmonic analysis, the classification began with 2000 voxels.
Sensitivity analysis then identified a much smaller subset of voxels that could be used for decoding with nearly the same accuracy as the full set.
This result could indicate that only a small number of voxels are relevant for classification, or that the information is highly redundant between voxels, or, most likely, a combination of the two.
Specifically, the harmonic analysis will select voxels that covary with the stimulus, but their patterns of activation may not be highly discriminative with respect to character count.
These voxels will also be assigned low sensitivity values by the trained classifiers.
On the other hand, brain responses themselves can show strong spatial correlations on the centimeter scale, particularly in the higher order visual areas that dominate the sensitivity results \citep{Engel1997}. 
Voxels sampling these regions could all show similar classification sensitivity, but yield redundant information.
Therefore, high sensitivity is sufficient but not necessary for the localization of a function in a particular region.
In any case, it is encouraging that the sensitivity maps were consistent across subjects.
Sensitivity analysis was only performed on the feedforward neural network classifier because of the existing literature on the topic \citep{Zurada1994}.
Applying a similar technique to the SVM is an avenue for future research.

When examined in conjunction with existing vision and cognitive neuroscience experiments, the sensitivity maps suggest how this information may be processed in the brain (Table \ref{tab:full-sensitivity}).
By far, the most sensitivity (42\%), and therefore the most relevant mutual information, is contained in the lateral occipital region (LO).
This region is most commonly associated with high-level object recognition \citep{Grill-Spector2001}, but also contains the extrastriate body area which has been implicated in the perception of body parts \citep{Astafiev2004}.
This observation is in contrast to the comparatively weak sensitivity (5.7\%) observed in the fusiform gyrus, which is also associated with object classification tasks and in particular the perception of human faces \citep{Kanwisher1997,Sayres2010}.
The superior temporal sulcus (STS), which has also been implicated in object selectivity \citep{Hasselmo1989,Beauchamp2004}, contained some sensitivity. 
Additionally, there is substantial sensitivity contained in low-level retinotopic visual areas. 
The pericalcarine, lingual, and cuneus areas altogether account for 25\% of the observed sensitivity; the more ventral lingual areas contribute more strongly (13.9\%) then cuneus (3.7\%).
This result is somewhat surprising given that the relevant information in the scene is not necessarily retinotopically organized because of the randomized arrangement of buildings and characters as well as the free-viewing paradigm.
However, the significant linear activation and sensitivity in early, retinotopic visual areas, together with the sensitivity in lateral-occipital areas suggests that retinotopic organization is important in decoding of group size. 
Because LO combines object-selectivity with retinotopic specificity \citep{Sayres2008}, different group sizes could evoke complex but stereotypical patterns of responses in LO (and other retinotopically organized areas) as subjects visually interrogate the stimuli. 
Despite the preponderance of retinotopic visual-area sensitivity, the dorsal parietal cortex is also substantially sensitive (16.6\%).
Such sensitivity suggests that the perception of motion and position are also useful for decoding subject number.
Some research also suggests that parts of the parietal cortex are involved in mental arithmetic and magnitude judgement \citep{Rickard2000} which may also play some role in decoding group size.

The maps of significant linear activation produced by GLM predominately covered early retinotopic visual areas with some linear activation in lateral occipital and fusiform regions.
In general, the maps of linear activation covered only a small subset of those regions found to be sensitive for discrimination.
The presence of sensitive regions that did not have significant linear activation along with the disparity in performance between the simple linear estimator and MVPA indicates that the performance of the MVPA can only partially be explained by simple linear and retinotopically organized activation.
While more extensive modeling of activation could lead to superior estimators, it would require a significant investment of time.
The advantage of the machine learning approach is that it tests a large range of models without user intervention.
In fact, the linear activation hypothesis used for the GLM was only constructed after viewing the results of the sensitivity analysis.
Once an initial working model is discovered, researchers can begin to refine that model to improve both performance and understanding of the underlying processes.

In general, the classification accuracy of a particular number of presented human characters decreased as the number of characters increased (Figure \ref{fig:average-confusion}).
This observation is in agreement with the logarithmic nature of perception; i.e., it is easier to tell the difference between one and two characters than it is to tell the difference between five and six characters \citep{Shepard1975,Dehaene2003}.
This phenomenon could also be related purely to retinotopic information, as indicated by the sensitivity analysis, rather than the perception of numbers.
Without a fixation point and with varying character group configurations, the retinotopic pattern of activation will vary from presentation to presentation even for the same character count.
However, the average activation in a retinotopically organized and object-selective region will increase for larger group sizes regardless of the group's configuration and the subject's fixation point.
The relative difference in average activity is smaller between larger presentation sizes and thus more difficult to differentiate.
However, there is an interesting exception to this trend that is difficult to account for with a retinotopic explanation; there is a significant increase in confusion between two and six characters.
This could be a result of six characters being too much for the subject to consider individually and the subject perceives two groups of characters rather than six individual characters.

While virtual environments offer a more realistic stimulus than traditional stimuli such as sweeping bars or contrast gratings, they are more complex to design and more difficult to interpret.
This paper offers some initial tools and analyses to deal with these challenges, but further work is needed.
Moreover, virtual environments do carry some disadvantages over simpler dynamic stimuli such as prerecorded movies. 
It is typically easier to record a movie than to program a virtual environment; movies can also have a more realistic appearance than virtual environments.
Additionally, the results of \cite{Han2005} indicate that there is some divergence in the way the brain processes real (i.e., recorded) and virtual visual worlds. 
However, it is likely that as the realism of virtual environments increases this divergence in processing will decrease as well.

Complex and dynamic stimuli allow researchers to examine neural processing in a more natural state, and virtual environments make it easier to design and control such stimuli.
MVPA allows researchers to explore the coding of information in the brain when an underlying neural processing model is not already known and to discover new and interesting interactions.
Used together, virtual environments and MVPA allow for novel and valuable experimental designs.
Altogether, the results in this paper suggest that the combination of virtual environments with machine-learning analysis schemes will offer a fruitful new tool for the analysis of brain responses in stimulus scenarios that more closely resemble natural human experience.

\section{References}
\bibliography{bib}

\end{document}
